{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Probabilistic Language Model (NPLM)\n",
    "\n",
    "This notebook implements a simplified version of the Neural Probabilistic Language Model originally proposed by Bengio et al. (2003).\n",
    "\n",
    "## Overview\n",
    "\n",
    "The NPLM predicts the next word in a sequence by:\n",
    "1. Looking at a fixed window of previous words (context)\n",
    "2. Embedding each word into a continuous vector space\n",
    "3. Concatenating these embeddings\n",
    "4. Passing them through a feed-forward neural network\n",
    "5. Outputting a probability distribution over the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Text Dataset\n",
    "\n",
    "We'll create a simple dataset for demonstration. In practice, you would use a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "text = \"\"\"\n",
    "the cat sat on the mat\n",
    "the dog sat on the log\n",
    "the cat and the dog played together\n",
    "cats and dogs are great pets\n",
    "the mat is on the floor\n",
    "dogs like to play outside\n",
    "cats like to sleep inside\n",
    "\"\"\".lower().strip()\n",
    "\n",
    "# Tokenize\n",
    "tokens = text.split()\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab = ['<PAD>', '<UNK>'] + sorted(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Data\n",
    "\n",
    "We'll create (context, target) pairs where context is a fixed window of previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(tokens, context_size=3):\n",
    "    \"\"\"\n",
    "    Create training data with fixed context window.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        context_size: Number of previous words to use as context\n",
    "    \n",
    "    Returns:\n",
    "        contexts: List of context sequences\n",
    "        targets: List of target words\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(context_size, len(tokens)):\n",
    "        context = tokens[i-context_size:i]\n",
    "        target = tokens[i]\n",
    "        contexts.append([word2idx[w] for w in context])\n",
    "        targets.append(word2idx[target])\n",
    "    \n",
    "    return torch.LongTensor(contexts), torch.LongTensor(targets)\n",
    "\n",
    "# Create training data\n",
    "CONTEXT_SIZE = 3\n",
    "X_train, y_train = create_training_data(tokens, CONTEXT_SIZE)\n",
    "\n",
    "print(f\"Number of training examples: {len(X_train)}\")\n",
    "print(f\"\\nFirst 5 training examples:\")\n",
    "for i in range(5):\n",
    "    context_words = [idx2word[idx.item()] for idx in X_train[i]]\n",
    "    target_word = idx2word[y_train[i].item()]\n",
    "    print(f\"Context: {context_words} -> Target: {target_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Probabilistic Language Model\n",
    "\n",
    "The model architecture:\n",
    "- **Embedding Layer**: Maps each word to a dense vector\n",
    "- **Concatenation**: Concatenates embeddings of context words\n",
    "- **Hidden Layer**: Feed-forward layer with tanh activation\n",
    "- **Output Layer**: Projects to vocabulary size for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        \"\"\"\n",
    "        Neural Probabilistic Language Model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            context_size: Number of context words\n",
    "            hidden_dim: Dimension of hidden layer\n",
    "        \"\"\"\n",
    "        super(NPLM, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        # Input: concatenated embeddings (context_size * embedding_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, context):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            context: Tensor of shape (batch_size, context_size)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embed each word in the context\n",
    "        embeds = self.embeddings(context)  # (batch_size, context_size, embedding_dim)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        embeds = embeds.view(-1, self.context_size * self.embedding_dim)\n",
    "        \n",
    "        # Pass through feed-forward network\n",
    "        hidden = torch.tanh(self.fc1(embeds))\n",
    "        logits = self.fc2(hidden)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Initialize model\n",
    "model = NPLM(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {num_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Prediction\n",
    "\n",
    "Let's test the model by predicting the next word given a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, context_words):\n",
    "    \"\"\"\n",
    "    Predict the next word given context words.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NPLM model\n",
    "        context_words: List of context words\n",
    "    \n",
    "    Returns:\n",
    "        predicted_word: The predicted next word\n",
    "        probabilities: Top-5 predictions with probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert words to indices\n",
    "    context_indices = [word2idx.get(w, word2idx['<UNK>']) for w in context_words]\n",
    "    context_tensor = torch.LongTensor([context_indices])\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(context_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get top-5 predictions\n",
    "    top_probs, top_indices = torch.topk(probs[0], k=min(5, vocab_size))\n",
    "    \n",
    "    predictions = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        word = idx2word[idx.item()]\n",
    "        predictions.append((word, prob.item()))\n",
    "    \n",
    "    return predictions[0][0], predictions\n",
    "\n",
    "# Test predictions\n",
    "test_contexts = [\n",
    "    ['the', 'cat', 'sat'],\n",
    "    ['the', 'dog', 'sat'],\n",
    "    ['cats', 'and', 'dogs'],\n",
    "    ['like', 'to', 'play']\n",
    "]\n",
    "\n",
    "print(\"Model Predictions:\\n\")\n",
    "for context in test_contexts:\n",
    "    predicted, top_preds = predict_next_word(model, context)\n",
    "    print(f\"Context: {' '.join(context)}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for word, prob in top_preds:\n",
    "        print(f\"  {word}: {prob:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, X, y):\n",
    "    \"\"\"\n",
    "    Calculate perplexity on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NPLM model\n",
    "        X: Context sequences\n",
    "        y: Target words\n",
    "    \n",
    "    Returns:\n",
    "        perplexity: The perplexity score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "perplexity = calculate_perplexity(model, X_train, y_train)\n",
    "print(f\"Training Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Text\n",
    "\n",
    "Generate text by repeatedly predicting the next word and using it as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_context, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate text by iteratively predicting next words.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NPLM model\n",
    "        start_context: Initial context words (list)\n",
    "        max_length: Maximum number of words to generate\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: The generated text as a string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    context = start_context.copy()\n",
    "    generated = start_context.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get the last context_size words\n",
    "        current_context = context[-CONTEXT_SIZE:]\n",
    "        \n",
    "        # Predict next word\n",
    "        predicted_word, _ = predict_next_word(model, current_context)\n",
    "        \n",
    "        # Add to generated text\n",
    "        generated.append(predicted_word)\n",
    "        context.append(predicted_word)\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Generate text from different starting contexts\n",
    "print(\"Generated Text:\\n\")\n",
    "\n",
    "start_contexts = [\n",
    "    ['the', 'cat', 'sat'],\n",
    "    ['the', 'dog', 'like'],\n",
    "    ['cats', 'and', 'dogs']\n",
    "]\n",
    "\n",
    "for start in start_contexts:\n",
    "    generated = generate_text(model, start, max_length=10)\n",
    "    print(f\"Start: {' '.join(start)}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Word Embeddings\n",
    "\n",
    "We can visualize the learned word embeddings in 2D space using t-SNE or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = model.embeddings.weight.detach().numpy()\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "\n",
    "# Add labels for each word\n",
    "for i, word in enumerate(vocab):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                 fontsize=9, alpha=0.7)\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Word Embeddings Visualization (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a simple implementation of the Neural Probabilistic Language Model (NPLM). Key concepts:\n",
    "\n",
    "1. **Word Embeddings**: Each word is mapped to a continuous vector space\n",
    "2. **Context Window**: Fixed-size window of previous words used for prediction\n",
    "3. **Feed-Forward Network**: Simple neural network processes concatenated embeddings\n",
    "4. **Next Word Prediction**: Model outputs probability distribution over vocabulary\n",
    "\n",
    "### Limitations of this Simple Model:\n",
    "- Fixed context size (cannot handle variable-length dependencies)\n",
    "- Small training corpus (would need much more data for real applications)\n",
    "- No regularization techniques (dropout, weight decay, etc.)\n",
    "- Simple architecture (modern models use more sophisticated architectures)\n",
    "\n",
    "### Extensions:\n",
    "- Use larger datasets (e.g., WikiText, Penn Treebank)\n",
    "- Add dropout for regularization\n",
    "- Implement the distant context aggregation from the paper\n",
    "- Compare with Transformer-based models\n",
    "- Add proper train/validation/test splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{\n "cells": [\n {\n "cell_type": "markdown",\n "metadata": {},\n "source": [\n "# Neural Probabilistic Language Model (Bengio et al., 2003)\n",\n "\n",\n "This notebook provides a **clean, simple implementation** of the Neural Probabilistic Language Model from:\n",\n "Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3, 1137-1155.\n",\n "\n",\n "The model learns distributed word representations and predicts P(w_t | w_{t-n+1}, ..., w_{t-1}).\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "import torch\n",\n "import torch.nn as nn\n",\n "import torch.nn.functional as F\n",\n "from torch.utils.data import Dataset, DataLoader\n",\n "from collections import Counter\n",\n "import numpy as np\n",\n "from typing import List, Tuple, Optional\n",\n "import matplotlib.pyplot as plt\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "class NeuralLanguageModel(nn.Module):\n",\n "    \"\"\"Neural Probabilistic Language Model (Bengio et al., 2003)\"\"\"\n",\n "    \"\"\"\n",\n "    def __init__(self, vocab_size: int, embedding_dim: int = 60, \n",\n "                 context_size: int = 4, hidden_dim: int = 100, \n",\n "                 use_direct_connections: bool = True):\n",\n "        super().__init__()\n",\n "        self.vocab_size = vocab_size\n",\n "        self.embedding_dim = embedding_dim\n",\n "        self.context_size = context_size\n",\n "        self.hidden_dim = hidden_dim\n",\n "        self.use_direct_connections = use_direct_connections\n",\n "        self.concat_dim = context_size * embedding_dim\n",\n "        \n",\n "        # C: Embedding matrix\n",\n "        self.C = nn.Embedding(vocab_size, embedding_dim)\n",\n "        \n",\n "        # Hidden layer: tanh(d + Hx)\n",\n "        self.H = nn.Linear(self.concat_dim, hidden_dim)\n",\n "        \n",\n "        # Output layer: U * tanh(...)\n",\n "        self.U = nn.Linear(hidden_dim, vocab_size)\n",\n "        \n",\n "        # Optional direct connections: Wx\n",\n "        if use_direct_connections:\n",\n "            self.W = nn.Linear(self.concat_dim, vocab_size, bias=False)\n",\n "        else:\n",\n "            self.W = None\n",\n "        \n",\n "        self._init_weights()\n",\n "    \n",\n "    def _init_weights(self):\n",\n "        nn.init.uniform_(self.C.weight, -0.01, 0.01)\n",\n "        nn.init.xavier_uniform_(self.H.weight)\n",\n "        nn.init.zeros_(self.H.bias)\n",\n "        nn.init.xavier_uniform_(self.U.weight)\n",\n "        nn.init.zeros_(self.U.bias)\n",\n "        if self.W is not None:\n",\n "            nn.init.xavier_uniform_(self.W.weight)\n",\n "    \n",\n "    def forward(self, context: torch.Tensor) -> torch.Tensor:\n",\n "        batch_size = context.size(0)\n",\n "        embeddings = self.C(context)\n",\n "        x = embeddings.view(batch_size, -1)\n",\n "        hidden = torch.tanh(self.H(x))\n",\n "        y = self.U(hidden)\n",\n "        if self.W is not None:\n",\n "            y = y + self.W(x)\n",\n "        return F.log_softmax(y, dim=1)\n",\n "    \n",\n "    def get_word_embeddings(self) -> torch.Tensor:\n",\n "        return self.C.weight.data\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "class Vocabulary:\n",\n "    PAD_TOKEN = "<PAD>"\n",\n "    UNK_TOKEN = "<UNK>"\n",\n "    \n",\n "    def __init__(self, min_freq: int = 1):\n",\n "        self.min_freq = min_freq\n",\n "        self.word2idx = {}\n",\n "        self.idx2word = {}\n",\n "        self.word_freq = Counter()\n",\n "    \n",\n "    def build(self, texts: List[List[str]]):\n",\n "        for tokens in texts:\n",\n "            self.word_freq.update(tokens)\n",\n "        self.word2idx[self.PAD_TOKEN] = 0\n",\n "        self.word2idx[self.UNK_TOKEN] = 1\n",\n "        idx = 2\n",\n "        for word, freq in self.word_freq.most_common():\n",\n "            if freq >= self.min_freq:\n",\n "                self.word2idx[word] = idx\n",\n "                idx += 1\n",\n "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "    def __len__(self):\n",\n "        return len(self.word2idx)\n",\n "    \n",\n "    def encode(self, word: str) -> int:\n",\n "        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n",\n "    \n",\n "    def decode(self, idx: int) -> str:\n",\n "        return self.idx2word.get(idx, self.UNK_TOKEN)\n",\n "    \n",\n "    def encode_sequence(self, words: List[str]) -> List[int]:\n",\n "        return [self.encode(word) for word in words]\n",\n "    \n",\n "    def decode_sequence(self, indices: List[int]) -> List[str]:\n",\n "        return [self.decode(idx) for idx in indices]\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "class NGramDataset(Dataset):\n",\n "    def __init__(self, texts: List[List[str]], vocab: Vocabulary, context_size: int):\n",\n "        self.vocab = vocab\n",\n "        self.context_size = context_size\n",\n "        self.data = []\n",\n "        \n",\n "        for tokens in texts:\n",\n "            indices = vocab.encode_sequence(tokens)\n",\n "            padded = [vocab.word2idx[Vocabulary.PAD_TOKEN]] * context_size + indices\n",\n "            for i in range(len(indices)):\n",\n "                context = padded[i:i + context_size]\n",\n "                target = indices[i]\n",\n "                self.data.append((context, target))\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "    def __len__(self):\n",\n "        return len(self.data)\n",\n "    \n",\n "    def __getitem__(self, idx):\n",\n "        context, target = self.data[idx]\n",\n "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "def train_epoch(model, dataloader, optimizer, device, clip_grad=5.0):\n",\n "    model.train()\n",\n "    total_loss = 0.0\n",\n "    num_batches = 0\n",\n "    \n",\n "    for context, target in dataloader:\n",\n "        context = context.to(device)\n",\n "        target = target.to(device)\n",\n "        optimizer.zero_grad()\n",\n "        log_probs = model(context)\n",\n "        loss = F.nll_loss(log_probs, target)\n",\n "        loss.backward()\n",\n "        if clip_grad is not None:\n",\n "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",\n "        optimizer.step()\n",\n "        total_loss += loss.item()\n",\n "        num_batches += 1\n    \n",\n "    return total_loss / num_batches\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "def evaluate(model, dataloader, device):\n",\n "    model.eval()\n",\n "    total_loss = 0.0\n",\n "    num_samples = 0\n",\n "    \n",\n "    with torch.no_grad():\n",\n "        for context, target in dataloader:\n",\n "            context = context.to(device)\n",\n "            target = target.to(device)\n",\n "            log_probs = model(context)\n",\n "            loss = F.nll_loss(log_probs, target, reduction='sum')\n",\n "            total_loss += loss.item()\n",\n "            num_samples += target.size(0)\n    \n",\n "    avg_loss = total_loss / num_samples\n",\n "    perplexity = np.exp(avg_loss)\n    return avg_loss, perplexity\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "def predict_next_word(model, vocab, context_words, top_k=5, device=torch.device('cpu')):\n",\n "    model.eval()\n",\n "    context_size = model.context_size\n",\n "    context_indices = vocab.encode_sequence(context_words)\n    \n",\n "    if len(context_indices) < context_size:\n",\n "        padding = [vocab.word2idx[Vocabulary.PAD_TOKEN]] * (context_size - len(context_indices))\n",\n "        context_indices = padding + context_indices\n",\n "    else:\n",\n "        context_indices = context_indices[-context_size:]\n    \n",\n "    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n    \n",\n "    with torch.no_grad():\n",\n "        log_probs = model(context_tensor)\n",\n "        probs = torch.exp(log_probs).squeeze()\n",\n "        top_probs, top_indices = torch.topk(probs, top_k)\n",\n "        predictions = [(vocab.decode(idx.item()), prob.item())  \n",\n "                      for prob, idx in zip(top_probs, top_indices)]\n    \n",\n "    return predictions\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Sample training data\n",\n "sample_texts = [\n",\n "    \"the cat sat on the mat\",\n",\n "    \"the dog ran in the park\",\n",\n "    \"a quick brown fox jumps over the lazy dog\",\n",\n "    \"the cat and the dog are friends\",\n",\n "    \"the sun is shining in the sky\",\n",\n "    \"birds are flying in the blue sky\",\n",\n "    \"the weather is nice today\",\n",\n "    \"i like to read books in the park\",\n",\n "    \"the park is beautiful in spring\",\n",\n "    \"flowers bloom in the garden\"\n",\n "]\n\n# Tokenize\ntrain_texts = [text.lower().split() for text in sample_texts]\n\n# Build vocabulary\nvocab = Vocabulary(min_freq=1)\nvocab.build(train_texts)\nprint(f\"Vocabulary size: {len(vocab)}\")\n\n# Create dataset\ncontext_size = 3\train_dataset = NGramDataset(train_texts, vocab, context_size)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nprint(f\"Training samples: {len(train_dataset)}\")\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Create and train model\n",\n "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",\n "print(f\"Using device: {device}\")\n",\n "\n",\n "model = NeuralLanguageModel(\n",\n "    vocab_size=len(vocab),\n",\n "    embedding_dim=30,\n",\n "    context_size=3,\n",\n "    hidden_dim=50,\n",\n "    use_direct_connections=True\n",\n ").to(device)\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",\n "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",\n "\n",\n "# Training loop\n",\n "losses = []\n",\n "num_epochs = 50\n",\n "\n",\n "for epoch in range(num_epochs):\n",\n "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",\n "    losses.append(train_loss)\n",\n "    scheduler.step()\n",\n "    if (epoch + 1) % 10 == 0:\n",\n "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}\")\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "print("Training complete!")\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Plot training loss\n",\n "plt.figure(figsize=(10, 5))\n",\n "plt.plot(losses)\n",\n "plt.xlabel('Epoch')\n",\n "plt.ylabel('Loss')\n",\n "plt.title('Training Loss')\n",\n "plt.grid(True)\n",\n "plt.show()\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Test predictions\n",\n "test_contexts = [\n",\n "    [\"the\", \"cat\", \"sat\"],\n",\n "    [\"in\", \"the\"],\n",\n "    [\"the\", \"dog\"]\n",\n "]\n"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "print("=== Predictions ===")\n",\n "for context in test_contexts:\n",\n "    predictions = predict_next_word(model, vocab, context, top_k=5, device=device)\n",\n "    print(f"\nContext: {' '.join(context)}")\n",\n "    print("Top predictions:")\n",\n "    for word, prob in predictions:\n",\n "        print(f"  {word}: {prob:.4f}")\n"\n ]\n },\n {\n "cell_type": "markdown",\n "metadata": {},\n "source": [\n "## Summary\n",\n "\n",\n "This simplified implementation includes:\n",\n "- **NeuralLanguageModel**: Core model with embedding (C), hidden layer (H), output layer (U), and optional direct connections (W)\n",\n "- **Vocabulary**: Word-to-index mapping\n",\n "- **NGramDataset**: Efficient n-gram dataset\n",\n "- **Training functions**: Simple, clean training and evaluation\n",\n "\n",\n "No unnecessary complexity - just the pure Bengio et al. 2003 architecture.\n"\n ]\n }\n ],\n "metadata": {\n "kernelspec": {\n "display_name": "Python 3",\n "language": "python",\n "name": "python3"\n },\n "language_info": {\n "codemirror_mode": {\n "name": "ipython",\n "version": 3\n },\n "file_extension": ".py",\n "mimetype": "text/x-python",\n "name": "python",\n "nbconvert_exporter": "python",\n "pygments_lexer": "ipython3",\n "version": "3.7.3"\n }\n }\n }\n
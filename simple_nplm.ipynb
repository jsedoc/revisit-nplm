{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Neural Probabilistic Language Model\n\n**Clean, simple implementation of:**\n\nBengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003).\nA Neural Probabilistic Language Model.\nJournal of Machine Learning Research, 3, 1137-1155.\n\nThe model learns a distributed representation for words and uses it to predict the probability distribution of the next word given the previous n-1 words.\n\n## Architecture\n\n1. **Embedding layer C**: maps each word index to a continuous vector\n2. **Hidden layer** with tanh activation\n3. **Output softmax layer** for word probabilities\n4. **Optional direct connections** from input to output\n\nThe probability function is:\n```\nP(w_t | w_{t-n+1}, ..., w_{t-1}) = softmax(y)\ny = b + Wx + U*tanh(d + Hx)\nx = (C(w_{t-n+1}), ..., C(w_{t-1}))  [concatenation of embeddings]\n```\n\nWhere:\n- C is the embedding matrix (|V| x m)\n- H is the hidden layer weight matrix (h x (n-1)*m)\n- d is the hidden layer bias (h)\n- U is the output layer weight matrix (|V| x h)\n- W is the optional direct connection matrix (|V| x (n-1)*m)\n- b is the output bias (|V|)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nimport numpy as np\nfrom typing import List, Tuple, Optional"]},{"cell_type":"markdown","metadata":{},"source":["## Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NeuralLanguageModel(nn.Module):\n    \"\"\"\n    Neural Probabilistic Language Model (Bengio et al., 2003)\n    \n    Args:\n        vocab_size: Size of vocabulary |V|\n        embedding_dim: Dimension of word embeddings (m in paper)\n        context_size: Number of context words (n-1 in paper)\n        hidden_dim: Dimension of hidden layer (h in paper)\n        use_direct_connections: Whether to use direct input-to-output connections (W matrix)\n    \"\"\"\n    \n    def __init__(\n        self,\n        vocab_size: int,\n        embedding_dim: int = 60,\n        context_size: int = 4,  # n-1, so this is a 5-gram model by default\n        hidden_dim: int = 100,\n        use_direct_connections: bool = True\n    ):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.context_size = context_size\n        self.hidden_dim = hidden_dim\n        self.use_direct_connections = use_direct_connections\n        \n        # Concatenated embedding dimension\n        self.concat_dim = context_size * embedding_dim\n        \n        # C: Embedding matrix (shared across all context positions)\n        self.C = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Hidden layer: y = tanh(d + Hx)\n        self.H = nn.Linear(self.concat_dim, hidden_dim)\n        \n        # Output layer from hidden: U * tanh(...)?\n        self.U = nn.Linear(hidden_dim, vocab_size)\n        \n        # Optional direct connections: Wx\n        if use_direct_connections:\n            self.W = nn.Linear(self.concat_dim, vocab_size, bias=False)\n        else:\n            self.W = None\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights following common practices\"\"\"\n        nn.init.uniform_(self.C.weight, -0.01, 0.01)\n        nn.init.xavier_uniform_(self.H.weight)\n        nn.init.zeros_(self.H.bias)\n        nn.init.xavier_uniform_(self.U.weight)\n        nn.init.zeros_(self.U.bias)\n        if self.W is not None:\n            nn.init.xavier_uniform_(self.W.weight)\n    \n    def forward(self, context: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the neural language model.\n        \n        Args:\n            context: Tensor of shape (batch_size, context_size) containing word indices\n        \n        Returns:\n            Tensor of shape (batch_size, vocab_size) containing log-probabilities\n        \"\"\"\n        batch_size = context.size(0)\n        \n        # Get embeddings for each context word and concatenate\n        # x = (C(w_{t-n+1}), ..., C(w_{t-1}))\n        embeddings = self.C(context)\n        x = embeddings.view(batch_size, -1)\n        \n        # Hidden layer with tanh activation: tanh(d + Hx)\n        hidden = torch.tanh(self.H(x))\n        \n        # Output: y = b + Wx + U*tanh(d + Hx)\n        y = self.U(hidden)\n        \n        # Add direct connections if enabled\n        if self.W is not None:\n            y = y + self.W(x)\n        \n        # Return log-softmax for numerical stability with NLLLoss\n        return F.log_softmax(y, dim=1)\n    \n    def get_word_embeddings(self) -> torch.Tensor:\n        \"\"\"Return the learned word embeddings (the C matrix)\"\"\"\n        return self.C.weight.data"]},{"cell_type":"markdown","metadata":{},"source":["## Vocabulary Management"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Vocabulary:\n    \"\"\"Vocabulary class for mapping words to indices and vice versa\"\"\"\n    \n    PAD_TOKEN = "<PAD>"\n    UNK_TOKEN = "<UNK>"\n    \n    def __init__(self, min_freq: int = 1):\n        self.min_freq = min_freq\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_freq = Counter()\n    \n    def build(self, texts: List[List[str]]):\n        \"\"\"Build vocabulary from tokenized texts\"\"\"\n        # Count word frequencies\n        for tokens in texts:\n            self.word_freq.update(tokens)\n        \n        # Add special tokens\n        self.word2idx[self.PAD_TOKEN] = 0\n        self.word2idx[self.UNK_TOKEN] = 1\n        \n        # Add words meeting minimum frequency threshold\n        idx = 2\n        for word, freq in self.word_freq.most_common():\n            if freq >= self.min_freq:\n                self.word2idx[word] = idx\n                idx += 1\n        \n        # Create reverse mapping\n        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def encode(self, word: str) -> int:\n        \"\"\"Convert word to index\"\"\"\n        return self.word2idx.get(word, self.word2idx[self.UNK_TOKEN])\n    \n    def decode(self, idx: int) -> str:\n        \"\"\"Convert index to word\"\"\"\n        return self.idx2word.get(idx, self.UNK_TOKEN)\n    \n    def encode_sequence(self, words: List[str]) -> List[int]:\n        \"\"\"Convert a list of words to indices\"\"\"\n        return [self.encode(word) for word in words]\n    \n    def decode_sequence(self, indices: List[int]) -> List[str]:\n        \"\"\"Convert a list of indices to words\"\"\"\n        return [self.decode(idx) for idx in indices]"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NGramDataset(Dataset):\n    \"\"\"Dataset for n-gram language modeling\"\"\"\n    \n    def __init__(self, texts: List[List[str]], vocab: Vocabulary, context_size: int):\n        self.vocab = vocab\n        self.context_size = context_size\n        self.data = []\n        \n        # Create n-grams from texts\n        for tokens in texts:\n            # Convert to indices\n            indices = vocab.encode_sequence(tokens)\n            \n            # Create context-target pairs\n            # Pad the beginning with PAD tokens\n            padded = [vocab.word2idx[Vocabulary.PAD_TOKEN]] * context_size + indices\n            \n            for i in range(len(indices)):\n                context = padded[i:i + context_size]\n                target = indices[i]\n                self.data.append((context, target))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        context, target = self.data[idx]\n        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Evaluation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize(text: str) -> List[str]:\n    \"\"\"Simple whitespace tokenizer with lowercasing\"\"\"\n    return text.lower().split()\n\n\ndef train_epoch(\n    model: NeuralLanguageModel,\n    dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    clip_grad: Optional[float] = 5.0\n) -> float:\n    \"\"\"Train for one epoch and return average loss\"\"\"\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n    \n    for context, target in dataloader:\n        context = context.to(device)\n        target = target.to(device)\n        \n        optimizer.zero_grad()\n        log_probs = model(context)\n        loss = F.nll_loss(log_probs, target)\n        loss.backward()\n        \n        if clip_grad is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n        \n        optimizer.step()\n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\n\ndef evaluate(\n    model: NeuralLanguageModel,\n    dataloader: DataLoader,\n    device: torch.device\n) -> Tuple[float, float]:\n    \"\"\"Evaluate the model and return average loss and perplexity\"\"\"\n    model.eval()\n    total_loss = 0.0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for context, target in dataloader:\n            context = context.to(device)\n            target = target.to(device)\n            log_probs = model(context)\n            loss = F.nll_loss(log_probs, target, reduction='sum')\n            total_loss += loss.item()\n            num_samples += target.size(0)\n    \n    avg_loss = total_loss / num_samples\n    perplexity = np.exp(avg_loss)\n    \n    return avg_loss, perplexity"]},{"cell_type":"markdown","metadata":{},"source":["## Inference Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_next_word(\n    model: NeuralLanguageModel,\n    vocab: Vocabulary,\n    context_words: List[str],\n    top_k: int = 5,\n    device: torch.device = torch.device('cpu')\n) -> List[Tuple[str, float]]:\n    \"\"\"\n    Predict the most likely next words given a context.\n    \n    Args:\n        model: Trained language model\n        vocab: Vocabulary\n        context_words: List of context words\n        top_k: Number of top predictions to return\n        device: Device to run inference on\n    \n    Returns:\n        List of (word, probability) tuples for top-k predictions\n    \"\"\"\n    model.eval()\n    context_size = model.context_size\n    \n    # Encode context words\n    context_indices = vocab.encode_sequence(context_words)\n    \n    # Pad or truncate to context_size\n    if len(context_indices) < context_size:\n        padding = [vocab.word2idx[Vocabulary.PAD_TOKEN]] * (context_size - len(context_indices))\n        context_indices = padding + context_indices\n    else:\n        context_indices = context_indices[-context_size:]\n    \n    context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n    \n    with torch.no_grad():\n        log_probs = model(context_tensor)\n        probs = torch.exp(log_probs).squeeze()\n        top_probs, top_indices = torch.topk(probs, top_k)\n        \n        predictions = []\n        for prob, idx in zip(top_probs.tolist(), top_indices.tolist()):\n            word = vocab.decode(idx)\n            predictions.append((word, prob))\n    \n    return predictions\n\ndef generate_text(\n    model: NeuralLanguageModel,\n    vocab: Vocabulary,\n    seed_words: List[str],\n    num_words: int = 20,\n    temperature: float = 1.0,\n    device: torch.device = torch.device('cpu')\n) -> List[str]:\n    \"\"\"\n    Generate text by sampling from the model.\n    \n    Args:\n        model: Trained language model\n        vocab: Vocabulary\n        seed_words: Initial words to start generation\n        num_words: Number of words to generate\n        temperature: Sampling temperature (higher = more random)\n        device: Device to run inference on\n    \n    Returns:\n        List of generated words (including seed words)\n    \"\"\"\n    model.eval()\n    context_size = model.context_size\n    generated = list(seed_words)\n    \n    for _ in range(num_words):\n        # Get context (last context_size words)\n        context_words = generated[-context_size:] if len(generated) >= context_size else generated\n        context_indices = vocab.encode_sequence(context_words)\n        \n        # Pad if necessary\n        if len(context_indices) < context_size:\n            padding = [vocab.word2idx[Vocabulary.PAD_TOKEN]] * (context_size - len(context_indices))\n            context_indices = padding + context_indices\n        \n        context_tensor = torch.tensor([context_indices], dtype=torch.long).to(device)\n        \n        with torch.no_grad():\n            log_probs = model(context_tensor)\n            \n            # Apply temperature\n            if temperature != 1.0:\n                log_probs = log_probs / temperature\n            \n            probs = torch.exp(log_probs).squeeze()\n            next_word_idx = torch.multinomial(probs, 1).item()\n            next_word = vocab.decode(next_word_idx)\n            generated.append(next_word)\n    \n    return generated"]},{"cell_type":"markdown","metadata":{},"source":["## Example: Training on Sample Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create sample training data\nsample_texts = [\n    \"the cat sat on the mat\",\n    \"the dog ran in the park\",\n    \"a quick brown fox jumps over the lazy dog\",\n    \"the cat and the dog are friends\",\n    \"the sun is shining in the sky\",\n    \"birds are flying in the blue sky\",\n    \"the weather is nice today\",\n    \"i like to read books in the park\",\n    \"the park is beautiful in spring\",\n    \"flowers bloom in the garden\",\n    \"the garden has many colorful flowers\",\n    \"children play in the park\",\n    \"the cat sleeps on the sofa\",\n    \"the dog plays with a ball\",\n    \"we went to the beach yesterday\",\n    \"the ocean waves are calm today\",\n    \"fish swim in the sea\",\n    \"the moon shines at night\",\n    \"stars twinkle in the dark sky\",\n    \"the night is quiet and peaceful\",\n]`]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tokenize\ntrain_texts = [tokenize(text) for text in sample_texts]\n\nprint(\"Sample tokenized texts:\")\nfor i, tokens in enumerate(train_texts[:3]):\n    print(f\"{i+1}. {tokens}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build vocabulary\nvocab = Vocabulary(min_freq=1)\nvocab.build(train_texts)\n\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Sample words: {list(vocab.word2idx.keys())[:15]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create dataset and dataloader\ncontext_size = 3\nbatch_size = 16\n\ntrain_dataset = NGramDataset(train_texts, vocab, context_size)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Number of batches: {len(train_loader)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = NeuralLanguageModel(\n    vocab_size=len(vocab),\n    embedding_dim=30,\n    context_size=context_size,\n    hidden_dim=50,\n    use_direct_connections=True\n).to(device)\n\nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel parameters: {num_params:,}\")\nprint(f\"\\nModel architecture:\")\nprint(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training\nnum_epochs = 50\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nprint(\"Starting training...\\n\")\nlosses = []\n\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, device)\n    losses.append(train_loss)\n    scheduler.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}\")\n\nprint(\"\\nTraining completed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot training curve\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.grid(True)\nplt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Testing: Next Word Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test predictions\ntest_contexts = [\n    [\"the\", \"cat\", \"sat\"],\n    [\"in\", \"the\"],\n    [\"the\", \"dog\"],\n]\n\nprint(\"=" * 60)\nprint(\"NEXT WORD PREDICTIONS\")\nprint(\"=" * 60)\n\nfor context in test_contexts:\n    predictions = predict_next_word(model, vocab, context, top_k=5, device=device)\n    print(f\"\\nContext: {' '.join(context)}\")\n    print(\"Top predictions:\")\n    for word, prob in predictions:\n        print(f\"  {word}: {prob:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Testing: Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\n\" + \"=" * 60)\nprint(\"TEXT GENERATION\")\nprint(\"=" * 60)\n\n# Generate text with different seeds\nseeds = [\n    [\"the\", \"cat\"],\n    [\"in\", \"the\"],\n    [\"the\", \"dog\"],\n]\n\nfor seed in seeds:\n    generated = generate_text(model, vocab, seed, num_words=10, temperature=0.8, device=device)\n    print(f\"\nSeed: {' '.join(seed)}\")\n    print(f\"Generated: {' '.join(generated)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Examining Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\n\" + \"=" * 60)\nprint(\"WORD EMBEDDINGS\")\nprint(\"=" * 60)\n\n# Get embeddings\nembeddings = model.get_word_embeddings()\nprint(f\"\nEmbedding matrix shape: {embeddings.shape}\")\nprint(f\"(vocab_size={len(vocab)}, embedding_dim={model.embedding_dim})\")\n\n# Cosine similarity function\ndef cosine_similarity(v1, v2):\n    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n\n# Find similar words\nif \"cat\" in vocab.word2idx:\n    cat_idx = vocab.word2idx[\"cat\"]\n    cat_embedding = embeddings[cat_idx]\n    \n    similarities = []\n    for word, idx in vocab.word2idx.items():\n        if word not in [Vocabulary.PAD_TOKEN, Vocabulary.UNK_TOKEN, \"cat\"]:\n            sim = cosine_similarity(cat_embedding, embeddings[idx]).item()\n            similarities.append((word, sim))\n    \n    similarities.sort(key=lambda x: x[1], reverse=True)\n    print(f\"\nWords most similar to 'cat':\")\n    for word, sim in similarities[:5]:\n        print(f\"  {word}: {sim:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n\nThis notebook demonstrates a **clean, simple implementation** of the Neural Probabilistic Language Model (Bengio et al., 2003) with:\n\n1. **Simple architecture**: Embedding layer, hidden layer with tanh, output softmax\n2. **Direct implementation**: Follows the mathematical formulation from the paper\n3. **No unnecessary complexity**: Minimal dependencies, straightforward code\n4. **Complete functionality**: Training, evaluation, prediction, and text generation\n\n### Key Components\n\n- `NeuralLanguageModel`: The core NPLM implementation\n- `Vocabulary`: Simple word-to-index mapping\n- `NGramDataset`: Creates context-target pairs for training\n- Training functions: Standard PyTorch training loop\n- Inference functions: Next word prediction and text generation\n\n### Model Architecture Details\n\n````\nInput: context words [w_{t-n+1}, ..., w_{t-1}]\n   ↓\nEmbedding Layer (C): Each word → embedding vector\n   ↓\nConcatenate: x = [e_1, e_2, ..., e_{n-1}]\n   ↓\nHidden Layer (H): tanh(Hx + d)\n   ↓\nOutput Layer (U): U * hidden\n   ↓\nDirect Connections (W): + W * x (optional)\n   ↓\nSoftmax: P(w_t | context)\n````\n\nThis implementation is faithful to the original paper while remaining easy to understand and modify."]}